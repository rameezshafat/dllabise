{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n\n# Load the WikiText dataset using Hugging Face\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\n# Preprocessing and Tokenization\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, sequence_length):\n        self.tokenizer = tokenizer\n        self.sequence_length = sequence_length\n        self.inputs = []\n        self.targets = []\n        \n        for text in texts:\n            tokens = tokenizer.encode(text, add_special_tokens=False)\n            for i in range(len(tokens) - sequence_length):\n                self.inputs.append(tokens[i:i+sequence_length])\n                self.targets.append(tokens[i+sequence_length])\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n\nsequence_length = 10\ntrain_texts = dataset['train']['text']\ntrain_dataset = TextDataset(train_texts, tokenizer, sequence_length)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Define the LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, _ = self.lstm(embedded)\n        output = self.fc(lstm_out[:, -1, :])\n        return output\n\nvocab_size = tokenizer.vocab_size\nembedding_dim = 128\nhidden_dim = 256\nnum_layers = 2\n\nmodel = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\n# Define Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef train_model(model, dataloader, criterion, optimizer, epochs):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for inputs, targets in dataloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n\n# Train the model\nprint(\"Training the model...\")\ntrain_model(model, train_loader, criterion, optimizer, epochs=5)\n\n# Prediction Function\ndef predict_next_word(model, tokenizer, text, sequence_length):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    tokens = tokenizer.encode(text, add_special_tokens=False)\n    tokens = tokens[-sequence_length:]\n    input_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        output = model(input_tensor)\n        predicted_index = torch.argmax(output, dim=1).item()\n        predicted_word = tokenizer.decode([predicted_index])\n        return predicted_word\n\n# Test the model with a custom input sequence\ninput_sequence = \"The history of artificial intelligence\"\npredicted_word = predict_next_word(model, tokenizer, input_sequence, sequence_length)\nprint(f\"Input: {input_sequence}\")\nprint(f\"Predicted next word: {predicted_word}\")\n","metadata":{"_uuid":"a7380b66-b63d-41ba-a021-2088cdb56121","_cell_guid":"7508a80c-50e1-4e0d-9513-85c52384eb28","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-09T07:16:30.490054Z","iopub.execute_input":"2024-12-09T07:16:30.490418Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbaa9d150ce54d0ab51c3a5440e5ddf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df36b8ac4b9e49a38990fe5e455c08d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61444da1d1e1403b9348bd8d0a4a4581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5b2156c77741da91c5699f7869605d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea988d2f8f5445d98bd677d073b833a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea9345fb1b94113b5693f361fe14858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ade1489d3f34bd4a1bbb97e5bfd6794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97edcafb6d7040b2bafc46b0ba26349c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e8fa022c62947aa99482c98d6972378"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bd181f2a55476580fc96ab7a55b6cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd602d14bb2d42d8a2f59923f4274fe9"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Training the model...\nEpoch 1, Loss: 6.1746\nEpoch 2, Loss: 5.5581\nEpoch 3, Loss: 5.2791\nEpoch 4, Loss: 5.0909\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install torchtext","metadata":{"_uuid":"890a0a66-413c-424b-9371-a00e4639366e","_cell_guid":"04c5bef3-7781-4bfd-8d4e-57344d212b8f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-09T07:11:51.945781Z","iopub.execute_input":"2024-12-09T07:11:51.946114Z","iopub.status.idle":"2024-12-09T07:12:01.774345Z","shell.execute_reply.started":"2024-12-09T07:11:51.946082Z","shell.execute_reply":"2024-12-09T07:12:01.773294Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting torchtext\n  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.32.3)\nRequirement already satisfied: torch>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext) (2024.6.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\nDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchtext\nSuccessfully installed torchtext-0.18.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"_uuid":"a9d54aa9-b986-4507-9e6d-9772b9c6108d","_cell_guid":"e88b7d0a-720a-4377-aaa2-0720da142cf2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-09T05:49:15.258788Z","iopub.status.idle":"2024-12-09T05:49:15.259362Z","shell.execute_reply.started":"2024-12-09T05:49:15.259085Z","shell.execute_reply":"2024-12-09T05:49:15.259108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"21d49add-ea43-41de-9716-af1f5930fe13","_cell_guid":"1016ded6-6a0b-429e-8525-9b81c919bcb3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-09T05:49:15.260654Z","iopub.status.idle":"2024-12-09T05:49:15.261080Z","shell.execute_reply.started":"2024-12-09T05:49:15.260855Z","shell.execute_reply":"2024-12-09T05:49:15.260877Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"5733f1fb-5822-47f8-b143-53e3006706ad","_cell_guid":"aa841f47-5700-41db-b14a-47a6d0f1eac3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-09T05:49:15.263030Z","iopub.status.idle":"2024-12-09T05:49:15.263498Z","shell.execute_reply.started":"2024-12-09T05:49:15.263237Z","shell.execute_reply":"2024-12-09T05:49:15.263275Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"8b153b8d-6707-46d1-b5fa-06cecbccc2da","_cell_guid":"2c0c31d6-8982-41b4-b0fa-ef180afdcdcc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2024-12-09T05:49:15.264430Z","iopub.status.idle":"2024-12-09T05:49:15.264857Z","shell.execute_reply.started":"2024-12-09T05:49:15.264628Z","shell.execute_reply":"2024-12-09T05:49:15.264652Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"9495734c-8ab9-4ed0-8127-1cbed29a875d","_cell_guid":"13911e50-7195-445b-bde0-753acda69541","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}